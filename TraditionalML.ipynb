{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Use basic ML methods for img recognition\n",
    "by Jenny Zeng & Ariel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import mltools as ml\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.learning_curve import validation_curve\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import learning_curve,GridSearchCV, ShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "from sklearn import tree, preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "denote number of data    \n",
    "mesa: 10015    \n",
    "forest: 10047   \n",
    "desert: 10142  \n",
    "training size: 21142  \n",
    "test size: 9062  \n",
    "total size:  30204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('/notebooks/Minecraft-AI/mc-data/mesa_forest_desert.txt', delimiter=';')\n",
    "np.random.shuffle(data)\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1].astype(np.int64)\n",
    "data_scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = data_scaler.transform(X)\n",
    "# Xtr, Ytr = X_scaled, Y\n",
    "Xtr, Ytr = X, Y\n",
    "#Xtr, Xte, Ytr, Yte = ml.splitData(X_scaled,Y,train_fraction=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('/notebooks/Minecraft-AI/mc-data/mesa_forest_desert_jungle_eh.txt', delimiter=';')\n",
    "np.random.shuffle(data)\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1].astype(np.int64)\n",
    "data_scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = data_scaler.transform(X)\n",
    "# Xtr, Ytr = X_scaled, Y\n",
    "Xtr, Ytr = X, Y\n",
    "#Xtr, Xte, Ytr, Yte = ml.splitData(X_scaled,Y,train_fraction=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.83655965 -0.80733804 -1.07026292 -0.67520534 -0.88809413 -0.84538711\n",
      "  1.77144905 -0.10544564  0.21200747  1.61789875  2.8005794   1.25396046\n",
      "  0.13615918  1.04476128  2.70497735 -0.62763659 -0.62426821 -0.29509756\n",
      " -0.30561289 -0.56256286 -0.25841537 -0.22344239 -0.27147997 -0.6475926 ]\n",
      "[  1.78300000e+03   3.93000000e+02   3.62000000e+02   3.54600000e+03\n",
      "   4.84100000e+03   1.96400000e+03   3.14850000e+04   7.04200000e+03\n",
      "   8.34700000e+03   1.82250000e+04   4.29870000e+04   1.30040000e+04\n",
      "   8.94200000e+03   8.73500000e+03   4.00000000e+04   1.90000000e+01\n",
      "   2.00000000e+00   3.08000000e+02   0.00000000e+00   0.00000000e+00\n",
      "   1.50000000e+01   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print X_scaled[0]\n",
    "print X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertLabel(lab):\n",
    "    return (numpy.arange(3) == lab[:, None]).astype(numpy.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show degree change cross validation curve\n",
    "def plot_validation_curve_lin(train_scores, test_scores, title, xlabel,param_range):\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    print(\"max mean test_score: \", test_scores_mean)\n",
    "    print(\"test_scores std: \", test_scores_std)\n",
    "    maxIndex = np.argmax(test_scores_mean)\n",
    "    print(\"best mean test_score:\", max(test_scores_mean),\" at\", maxIndex, \" that is\", param_range[maxIndex])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Score\")\n",
    "#     plt.ylim(0.0, 1.1)\n",
    "    lw = 2\n",
    "    plt.plot(param_range, train_scores_mean, label=\"Training score\",\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                     color=\"darkorange\", lw=lw)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "                 color=\"navy\", lw=lw)\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                     color=\"navy\", lw=lw)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show degree change cross validation curve\n",
    "def plot_validation_curve_log(train_scores, test_scores, title, xlabel, param_range, logbase=10):\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    print(\"max mean test_score: \", test_scores_mean)\n",
    "    maxIndex = np.argmax(test_scores_mean)\n",
    "    print(\"best mean test_score:\", max(test_scores_mean),\" at\", maxIndex, \" that is\", param_range[maxIndex])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    lw = 2\n",
    "    plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "                 color=\"darkorange\", lw=lw,basex=logbase)\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                     color=\"darkorange\", lw=lw)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "                 color=\"navy\", lw=lw,basex=logbase)\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                     color=\"navy\", lw=lw)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, scoring=make_scorer(roc_auc_score),\n",
    "        n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_range =np.arange(8,30)\n",
    "train_scores, test_scores = validation_curve(\n",
    "    tree.DecisionTreeClassifier(), Xtr, Ytr, param_name=\"max_depth\", param_range=param_range,\n",
    "    cv=10, scoring=make_scorer(roc_auc_score), n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_validation_curve_lin(train_scores, test_scores, \"Validation curve Decision Tree (max_depth)\", \"max_depth\",param_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"max_depth\":[13]}\n",
    "svr = tree.DecisionTreeClassifier()\n",
    "clf = GridSearchCV(svr, parameters,scoring=make_scorer(roc_auc_score),n_jobs=3)\n",
    "clf.fit(Xtr, Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_range =np.linspace(330,360,5, dtype=\"int\")\n",
    "train_scores, test_scores = validation_curve(\n",
    "    RandomForestClassifier(criterion=\"gini\",\n",
    "    max_features=4,\n",
    "    max_depth=14,\n",
    "    min_samples_split=8,\n",
    "    min_samples_leaf=4,\n",
    "    class_weight=\"balanced\"\n",
    "                          ), Xtr, Ytr, param_name=\"n_estimators\", param_range=param_range,\n",
    "    cv=10, scoring=make_scorer(roc_auc_score), n_jobs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_curve_lin(train_scores, test_scores, \"Cross validation curve Random forest (n_estimators)\", \"n_estimators\",param_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_range =np.linspace(284,290,5,dtype='int')\n",
    "train_scores, test_scores = validation_curve(\n",
    "    GradientBoostingClassifier(\n",
    "        loss='deviance', \n",
    "        subsample=0.83, \n",
    "        min_impurity_split=1e-04,\n",
    "        max_features=8,\n",
    "        max_depth=14,\n",
    "        min_samples_split=14,\n",
    "        min_samples_leaf=8,\n",
    "        learning_rate=0.15,\n",
    "        random_state=1\n",
    "    ), Xtr, Ytr, param_name=\"n_estimators\", param_range=param_range,\n",
    "    cv=10, scoring=make_scorer(roc_auc_score), n_jobs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_curve_lin(train_scores, test_scores, \"Cross validation curve GradientBoosting (n_estimators)\", \"n_estimators\",param_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "NUM_LABELS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate and confusions.\"\"\"\n",
    "    correct = numpy.sum(numpy.argmax(predictions, 1) == numpy.argmax(labels, 1))\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    error = 100.0 - (100 * float(correct) / float(total))\n",
    "\n",
    "    confusions = numpy.zeros([NUM_LABELS, NUM_LABELS], numpy.float32)\n",
    "    bundled = zip(numpy.argmax(predictions, 1), numpy.argmax(labels, 1))\n",
    "    for predicted, actual in bundled:\n",
    "        confusions[predicted, actual] += 1\n",
    "    \n",
    "    return error, confusions\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "multi_target_forest = MultiOutputClassifier(forest, n_jobs=3)\n",
    "result=multi_target_forest.fit(Xtr, convertLabel(Ytr)).predict(Xtr)\n",
    "\n",
    "\n",
    "#param_range =np.linspace(330,360,5, dtype=\"int\")\n",
    "#train_scores, test_scores = validation_curve(\n",
    " #   MultiOutputClassifier(\n",
    "  #  RandomForestClassifier(criterion=\"gini\",\n",
    "   # max_features=4,\n",
    "    #max_depth=14,\n",
    "    #min_samples_split=8,\n",
    "    #min_samples_leaf=4,\n",
    "    #n_estimators=100,\n",
    "    #class_weight=\"balanced\"), n_jobs=3),\n",
    "    #Xtr, convertLabel(Ytr),\n",
    "    #cv=10, scoring=make_scorer(roc_auc_score))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030179264833108732"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_rate(result, convertLabel(Ytr))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Yhat_sklearn.txt',\n",
    "    np.vstack( (np.arange(len(Ytr)) , Ytr) ).T,\n",
    "        '%d, %.2f',header='Index,Prob1',comments='',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More---Ensemble,gradient boosting and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn import grid_search\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error_(ground_truth, predictions):\n",
    "    return mean_squared_error(ground_truth, predictions) ** 0.5\n",
    "\n",
    "RMSE = make_scorer(mean_squared_error_, greater_is_better=False)\n",
    "\n",
    "\n",
    "class Ensemble(object):\n",
    "    def __init__(self, n_folds, stacker, base_models):\n",
    "        self.n_folds = n_folds\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            print('Fitting For Base Model #%d / %d ---', i+1, len(self.base_models))\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "\n",
    "                print('--- Fitting For Fold %d / %d ---', j+1, self.n_folds)\n",
    "\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                # y_holdout = y[test_idx]\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_holdout)[:]\n",
    "                S_train[test_idx, i] = y_pred\n",
    "\n",
    "                print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "            print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        print('--- Base Models Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        clf = self.stacker\n",
    "        clf.fit(S_train, y)\n",
    "\n",
    "        print('--- Stacker Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "    def preidct(self, X):\n",
    "        X = np.array(X)\n",
    "        folds = list(KFold(len(X), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "        S_test = np.zeros((X.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "            S_test_i = np.zeros((X.shape[0], len(folds)))\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                S_test_i[:, j] = clf.predict(X)[:]\n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "\n",
    "        clf = self.stacker\n",
    "        y_pred = clf.predict(S_test)[:]\n",
    "        return y_pred\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            print('Fitting For Base Model #{0} / {1} ---'.format(i+1, len(self.base_models)))\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], len(folds)))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "\n",
    "                print('--- Fitting For Fold #{0} / {1} ---'.format(j+1, self.n_folds))\n",
    "\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                # y_holdout = y[test_idx]\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_holdout)[:]\n",
    "                S_train[test_idx, i] = y_pred\n",
    "                S_test_i[:, j] = clf.predict(T)[:]\n",
    "\n",
    "                print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "\n",
    "            print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        print('--- Base Models Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        param_grid = {\n",
    "             'n_estimators': [100,200],\n",
    "             'learning_rate': [0.05, 0.055],\n",
    "             'subsample': [0.74, 0.75]\n",
    "         }\n",
    "\n",
    "        grid = grid_search.GridSearchCV(estimator=self.stacker, param_grid=param_grid, n_jobs=1, cv=5, verbose=20, scoring=RMSE)\n",
    "        grid.fit(S_train, y)\n",
    "\n",
    "        # a little memo\n",
    "        message = 'to determine local CV score of '\n",
    "\n",
    "        try:\n",
    "            print('Param grid:')\n",
    "            print(param_grid)\n",
    "            print('Best Params:')\n",
    "            print(grid.best_params_)\n",
    "            print('Best CV Score:')\n",
    "            print(-grid.best_score_)\n",
    "            print('Best estimator:')\n",
    "            print(grid.best_estimator_)\n",
    "            print(message)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print('--- Stacker Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        y_pred = grid.predict(S_test)[:]\n",
    "\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    print('--- Features Set: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "#    print('Number of Features: ', len(X_train.columns.tolist()))\n",
    "\n",
    "    base_models = [\n",
    "        RandomForestRegressor(\n",
    "            n_jobs=1, random_state=2016, verbose=1,\n",
    "            max_features=4,\n",
    "            max_depth=14,\n",
    "            min_samples_split=8,\n",
    "            min_samples_leaf=4,\n",
    "            n_estimators=352,\n",
    "        ),\n",
    "      \n",
    "       # ExtraTreesRegressor(\n",
    "        #    n_jobs=1, random_state=2016, verbose=1,\n",
    "         #   n_estimators=50, max_features=12\n",
    "        #),\n",
    "      #  GradientBoostingRegressor(\n",
    "       #     random_state=2016, verbose=1,#MAX_FEATURE BEST IS 3 ACCORDING TO THE SECOND, TRY THIS\n",
    "        #    n_estimators=200, max_features=12, max_depth=8,#change the n_estimator to 200 also,according to the result of the second \n",
    "         #   learning_rate=0.05, subsample=0.75 #change from 0.8 to 0.75 according to the result of the second \n",
    "        #),\n",
    "       GradientBoostingRegressor(\n",
    "           subsample=0.83, \n",
    "        min_impurity_split=1e-04,\n",
    "        max_features=8,\n",
    "        max_depth=14,\n",
    "        min_samples_split=14,\n",
    "        min_samples_leaf=8,\n",
    "        learning_rate=0.15,\n",
    "        random_state=2016,\n",
    "           \n",
    "            verbose=1,\n",
    "            n_estimators=287\n",
    "       )\n",
    "        #uptonow this one is the best \n",
    "        \n",
    "     #   XGBRegressor(\n",
    "      #      seed=2016,\n",
    "       #     n_estimators=200, max_depth=8,\n",
    "        #    learning_rate=0.05, subsample=0.8, colsample_bytree=0.85\n",
    "        #)\n",
    "    ]\n",
    "    ensemble = Ensemble(\n",
    "        n_folds=5,\n",
    "        stacker=GradientBoostingRegressor(\n",
    "            random_state=2016, verbose=1\n",
    "        ),\n",
    "        #stacker=XGBRegressor(\n",
    "         #   seed=2016\n",
    "            #seed=2016,\n",
    "            #n_estimators=200, max_depth=8,\n",
    "            #learning_rate=0.05, subsample=0.8, colsample_bytree=0.85\n",
    "        #),\n",
    "        base_models=base_models\n",
    "    )\n",
    "\n",
    "    yy_pred = ensemble.fit_predict(X=X_train, y=Y_train, T=X_validation)\n",
    "    print yy_pred\n",
    "    #np.savetxt('Yhat_staking_sklearn8.txt',\n",
    "    #np.vstack( (np.arange(len(yy_pred)) , yy_pred) ).T,\n",
    "     #   '%d, %.2f',header='ID,Prob1',comments='',delimiter=',')\n",
    "    \n",
    "   # pd.DataFrame({'id': id_test, 'relevance': y_pred}).to_csv('submission_ensemble.csv', index=False)\n",
    "\n",
    "    print('--- Submission Generated: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(Xtr,Ytr,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
